{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzD6BHYvTO1B"
      },
      "source": [
        "#Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ_3XuFTT0NJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c33040-9127-4d21-e2b6-df1038b07774"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!pip3 install pyspark\n",
        "!pip3 install findspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tar: spark-3.1.1-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 73kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 19.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=749ed4dd5d016f826f73ccc8535096b3fd3bacb9d59e8e13f32ba28297eebb05\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n",
            "Collecting findspark\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/2d/2e39f9a023479ea798eed4351cd66f163ce61e00c717e03c37109f00c0f2/findspark-1.4.2-py2.py3-none-any.whl\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-1.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqhRjAseKiPv"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-VMs2sZTYGf"
      },
      "source": [
        "#For DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qed8km8rKwYl"
      },
      "source": [
        "\"\"\"\n",
        "import findspark\n",
        "findspark.init('/content/spark-2.4.8-bin-hadoop2.7')\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\"\"\"\n",
        "#from pyspark.sql import SparkSession\n",
        "#spark=SparkSession.builder.appName('date').getOrCreate()\n",
        "# Let Spark know about the header and infer the Schema types!\n",
        "#base_path='/content/drive/MyDrive/BigData/Python_and_Spark_for_Big_Data/Spark_DataFrames/'\n",
        "#base_path = \"/data/\"\n",
        "#df = spark.read.csv(base_path+'appl_stock.csv',inferSchema=True,header=True)\n",
        "#df.printSchema()\n",
        "#df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AweffjIETgTM"
      },
      "source": [
        "#Rating Histogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsBuyUgaLtpL"
      },
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "import collections\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"RatingsHistogram\")\n",
        "#start\n",
        "sc = SparkContext(conf = conf)\n",
        "#data_path=\"file:////content/drive/MyDrive/WorkSpace/BigData/Taming_BigData_with_Apache_Spark_and_Python/ml-100k/u.data\"\n",
        "data_path =  \"/data/u.data\"\n",
        "lines = sc.textFile(data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXFBzf2MRmaw"
      },
      "source": [
        "ratings = lines.map(lambda x: x.split()[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PSk3mJaQck3",
        "outputId": "63bd6bcd-1664-4404-b9c3-903ca48bfda1"
      },
      "source": [
        "#sc.stop()\n",
        "result = ratings.countByValue()\n",
        "print(result)\n",
        "print(sorted(result.items()))\n",
        "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
        "sortedResults"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'int'>, {'3': 27145, '1': 6110, '2': 11370, '4': 34174, '5': 21201})\n",
            "[('1', 6110), ('2', 11370), ('3', 27145), ('4', 34174), ('5', 21201)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('1', 6110),\n",
              "             ('2', 11370),\n",
              "             ('3', 27145),\n",
              "             ('4', 34174),\n",
              "             ('5', 21201)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltLOq2fgP83G",
        "outputId": "0fd5cc28-7165-49b1-ef55-57bafb5a2703"
      },
      "source": [
        "for key, value in sortedResults.items():\n",
        "    print(\"%s %i\" % (key, value))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 6110\n",
            "2 11370\n",
            "3 27145\n",
            "4 34174\n",
            "5 21201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3z3r_3RTphx"
      },
      "source": [
        "#Friends by age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NihEYmIzm8pz"
      },
      "source": [
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrVtNIvIUDZM",
        "outputId": "398233f3-abb8-44a3-a316-5e849c3d8562"
      },
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"FriendsByAge\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "def parseLine(line):\n",
        "    fields = line.split(',')\n",
        "    age = int(fields[2])\n",
        "    numFriends = int(fields[3])\n",
        "    return (age, numFriends)\n",
        "#DataPath=\"file:////content/drive/MyDrive/WorkSpace/BigData/Taming_BigData_with_Apache_Spark_and_Python/Datasets/fakefriends.csv\"\n",
        "DataPath =  \"/data/fakefriends.csv\"\n",
        "lines = sc.textFile(DataPath)\n",
        "rdd = lines.map(parseLine)\n",
        "totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
        "averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])\n",
        "results = averagesByAge.collect()\n",
        "for result in results:\n",
        "    print(result)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(33, 325.3333333333333)\n",
            "(26, 242.05882352941177)\n",
            "(55, 295.53846153846155)\n",
            "(40, 250.8235294117647)\n",
            "(68, 269.6)\n",
            "(59, 220.0)\n",
            "(37, 249.33333333333334)\n",
            "(54, 278.0769230769231)\n",
            "(38, 193.53333333333333)\n",
            "(27, 228.125)\n",
            "(53, 222.85714285714286)\n",
            "(57, 258.8333333333333)\n",
            "(56, 306.6666666666667)\n",
            "(43, 230.57142857142858)\n",
            "(36, 246.6)\n",
            "(22, 206.42857142857142)\n",
            "(35, 211.625)\n",
            "(45, 309.53846153846155)\n",
            "(60, 202.71428571428572)\n",
            "(67, 214.625)\n",
            "(19, 213.27272727272728)\n",
            "(30, 235.8181818181818)\n",
            "(51, 302.14285714285717)\n",
            "(25, 197.45454545454547)\n",
            "(21, 350.875)\n",
            "(42, 303.5)\n",
            "(49, 184.66666666666666)\n",
            "(48, 281.4)\n",
            "(50, 254.6)\n",
            "(39, 169.28571428571428)\n",
            "(32, 207.9090909090909)\n",
            "(58, 116.54545454545455)\n",
            "(64, 281.3333333333333)\n",
            "(31, 267.25)\n",
            "(52, 340.6363636363636)\n",
            "(24, 233.8)\n",
            "(20, 165.0)\n",
            "(62, 220.76923076923077)\n",
            "(41, 268.55555555555554)\n",
            "(44, 282.1666666666667)\n",
            "(69, 235.2)\n",
            "(65, 298.2)\n",
            "(61, 256.22222222222223)\n",
            "(28, 209.1)\n",
            "(66, 276.44444444444446)\n",
            "(46, 223.69230769230768)\n",
            "(29, 215.91666666666666)\n",
            "(18, 343.375)\n",
            "(47, 233.22222222222223)\n",
            "(34, 245.5)\n",
            "(63, 384.0)\n",
            "(23, 246.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkoKY5bumA3d"
      },
      "source": [
        "#Nxt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meHgFikPmCtS"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahbn5gS9mAHw"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}